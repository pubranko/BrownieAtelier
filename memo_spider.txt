
custom_settings
crawler
    この属性はクラスの初期化後に、 from_crawler() クラスメソッドによってセットされこのSpiderインスタンスがどの Crawler オブジェクトにリンクされているかを持ちます。
    クローラは、単一のエントリーアクセス(extension, middleware, signal manager等)のため、プロジェクト内のたくさんのコンポーネントをカプセル化しています。
    詳細については Crawler API を参照してください。
logger
    Spiderの name でPythonのloggerが作成されます。ログメッセージの送信はこれを介して行うことができます。 詳しくは Logging from Spiders を参照してください。
log(message[, level, component])
    Spiderの logger を通じてログメッセージを送るための下位互換を保つために設けられたラッパーです。詳しくは Logging from Spiders を確認してください。
from_crawler(crawler, *args, **kwargs)
start_requests()
    これはScrapyによってSpiderがクロールするために起動する際に呼び出されます。Scrapyはこれを一度だけ呼び出すため、
    start_requests() をジェネレータとして安全に実装することができます。
    デフォルトの実装では start_urls 内の各URLに対して Request(url, dont_filter=True) を生成します。
    もし、ドメインのクロール開始のリクエストを変更したい場合は、このメソッドをオーバライドします。
    例えば、最初にPOSTリクエストでログインする必要がある場合は、以下のようにします。
closed(reason)
    スパイダーの終了時に呼ばれます。このメソッドは spider_closed シグナルを送るためのsignals.connect() のショートカットを提供します。
sitemap_urls
    クロールしたいSitemapを指すURLのリストです。
    robots.txt を指定することもできます。その場合、その解析結果に含まれるSitemapが利用されます。
sitemap_alternate_links
    url の代替リンクに従うかどうかを指定します。これらのリンクは、同じ url ブロック内で渡された別の言語の同じWEBサイトへのリンクです。
sitemap_follow = ['/photo/daily/news/180329/',]    #サイトマップを限定したい場合、r'post-2015-'のように、正規表現で指定できる。
sitemap_rules = [(r'/dly1804250003-n\d','parse'),] #実際のサイトへのリンクを絞り込み。
sitemap_rules = [   #実際のサイトへのリンクを絞り込み。
    (r'/politics/news/\d{6}/.+$','parse'),
    (r'/affairs/news/\d{6}/.+$','parse'),
    (r'/world/news/\d{6}/.+$','parse'),
    (r'/economy/news/\d{6}/.+$','parse'),
    (r'/column/news/\d{6}/.+$','parse'),
]
sitemap_filter
    親クラスのSitemapSpiderの同名メソッドをオーバーライド。
    entriesには、サイトマップから取得した情報(loc,lastmodなど）が辞書形式で格納されている。
    サイトマップから取得したurlへrequestを行う前に条件で除外することができる。
    対象のurlの場合、”yield entry”で返すと親クラス側でrequestされる。

